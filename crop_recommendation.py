# -*- coding: utf-8 -*-
"""crop_recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12WGQ1MkXUY3Qqc5wZqKN74S82zuqOXVa

CROP RECOMMENDATION


Precision agriculture is in trend nowadays. It helps the farmers to get informed decision about the farming strategy.

Data fields

*   N - ratio of Nitrogen content in soil
*   P - ratio of Phosphorous content in soil
*   K - ratio of Potassium content in soil
*   temperature - temperature in degree Celsius
*   humidity - relative humidity in %
*   ph - ph value of the soil
*   rainfall - rainfall in mm
*   label - which crop is recommended










---
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import chi2_contingency
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy.stats import f_oneway
import seaborn as sns
import math
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

crop = pd.read_csv("/content/Crop_recommendation.csv")

"""T-TEST"""

from scipy import stats

xbar = 50.55 ; mu0 = 140; s = 36.9; n = 30

#xbar =avg N ; muo= 140 ; s=36.9 ; n= 30

t_smple = (xbar-mu0)/(s/np.sqrt(float(n)));
 print ("Test Statistic:",round(t_smple,2))

alpha = 0.05
 t_alpha = stats.t.ppf(alpha,n-1);

print ("Critical value from t-table:",round(t_alpha,3))

p_val = stats.t.sf(np.abs(t_smple), n-1);

print ("Lower tail p-value from t-table", p_val)

crop1=crop['rainfall'];crop1

stats.ttest_1samp(crop1,155)

#no significance difference . null hypothesis is accepted
#variation in rainfall levels does not have a significant effect on crop yield or crop suitability.

"""# New Section

Feature Engineering
"""

crop.head()

crop.describe()

def mean(df):
    return sum(crop.N)/len(crop)
print(mean(crop))

def median(crop):
    median = sorted(crop) [len(crop)// 2]
    return median

sort_data = sorted(crop)
index1 = len(sort_data)*.25
print(index1)
#50th percentile
sort_data = sorted(crop)
index2 = len(sort_data)*.50
print(index2)
#75th percentile
sort_data = sorted(crop)
index3 = len(sort_data)*.75
print(index3)

print(np.std(crop))

modes = crop.mode().iloc[0:3]

# Display the modes for each field
print("Modes for each field:")
print(modes)

crop.columns

y=['label']
x=[['N','P','K','temperature','humidity','ph','rainfall']]

crop.shape

"""NULL VALUES"""

crop.info()

crop.isnull().sum()

crop.duplicated().sum()

"""Printing the unique values of 'label' column"""

df = pd.DataFrame(crop)
df.label.unique()

df

crop.nunique

corr=crop.corr()

corr

fig, ax = plt.subplots(1, 1, figsize=(15, 9))
sns.heatmap(crop.corr(), annot=True,cmap='Wistia')
ax.set(xlabel='features')
ax.set(ylabel='features')

plt.title('Correlation between different features', fontsize = 15, c='black')
plt.show()

"""ENCODING"""

crop_dictionary = {
    'rice':1,
    'maize':2,
    'chickpea':3,
    'kidneybeans':4,
    'pigeonpeas':5,
    'mothbeans':6,
    'mungbean':7,
    'blackgram':8,
    'lentil':9,
    'pomegranate':10,
    'banana':11,
    'mango':12,
    'grapes':13,
    'watermelon':14,
    'muskmelon':15,
    'apple':16,
    'orange':17,
    'papaya':18,
    'coconut':19,
    'cotton':20,
    'jute':21,
    'coffee':22
}
crop['crop_num']=crop['label'].map(crop_dictionary)

crop['crop_num'].value_counts()

crop.head()

y=crop['crop_num']
X=crop[['N','P','K','temperature','humidity','ph','rainfall']]

"""feature selection using F_score"""

#calculates the ratio of between-group variation to within-group variation.
#helping to identify the most informative features for classification or regression tasks.
from sklearn.feature_selection import f_classif,SelectKBest
fs=SelectKBest(score_func=f_classif,k=5)

fs.fit(X,y)

fs.get_support()# retrieve the selected features from a feature selection process like SelectKBest

fs.get_support().sum()

def list_ceil(X):
  return[math.ceil(i) for i in X]

print("F values:")
print(fs.scores_)

list_ceil(fs.scores_)

features_score=pd.DataFrame(fs.scores_)

features=pd.DataFrame(X.columns)
feature_score=pd.concat([features,features_score],axis=1)
feature_score.columns=["Input_Features","Score"]
print(feature_score.nlargest(5,columns="Score"))

"""FEATURE SCALING"""

crop.head(5)

'''scaler=MinMaxScaler()
x_scaled= scaler.fit_transform(x_array)
x_scaled'''

"""Testing

ANOVA ANALYSIS
"""

import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols

ano=ols('temperature~rainfall',data=crop).fit()

one=sm.stats.anova_lm(ano,type=2)
one

ano2=ols('crop_num~ph+temperature',data=crop).fit()
two=sm.stats.anova_lm(ano2,type=2)

two

"""CHI SQUARE TEST"""

from scipy.stats import chi2_contingency
df = pd.DataFrame(crop)
contingency_table = pd.crosstab(df['humidity'], df['rainfall'])
chi2, p, dof, expected = chi2_contingency(contingency_table)
alpha = 0.05  # Significance level
if p < alpha:
    print("There is a significant association between humidity and rainfall.")
else:
    print("There is no significant association between humidity and rainfall.")



"""KNN"""

import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,mean_absolute_percentage_error,mean_squared_error,r2_score

missing=crop[crop.isnull().any(axis=1)]

crop2=crop.dropna(axis=0)#It helps ensure a clean and complete dataset before training a model, preventing issues with missing data during analysis.

crop2['label']=crop2['label'].map({'rice':1,'maize':2,'chickpea':3,'kidneybeans':4,'pigeonpeas':5, 'mothbeans':6,
    'mungbean':7,
    'blackgram':8,
    'lentil':9,
    'pomegranate':10,
    'banana':11,
    'mango':12,
    'grapes':13,
    'watermelon':14,
    'muskmelon':15,
    'apple':16,
    'orange':17,
    'papaya':18,
    'coconut':19,
    'cotton':20,
    'jute':21,
    'coffee':22})

print(crop2['label'])

new_crop=pd.get_dummies(crop2,drop_first=True)

columns_list=list(new_crop.columns)

print(columns_list)

features=list(set(columns_list)-set(['label']))

features

y=new_crop['label'].values

y

x=new_crop[features].values

x

train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.3,random_state=0)

KNN_classifier=KNeighborsClassifier(n_neighbors=5)
#takes the most similar hhistoric instances to provide recommendations

KNN_classifier.fit(train_x,train_y)

prediction=KNN_classifier.predict(test_x)

confusionMmatrix=confusion_matrix(test_y,prediction)

confusionMmatrix

accuracy = accuracy_score(test_y, prediction)

print("Accuracy Score:", accuracy)

f1=f1_score(test_y,prediction,average='micro')

f1

"""LINEAR REGRESSION"""

from sklearn.linear_model import LinearRegression
model=LinearRegression()

model.fit(train_x,train_y)

#understand the relationship between a dependent variable and one or more independent variables
# making predictions, and identifying the impact of certain factors on the target variable.

model.intercept_
#It gives the estimated value of the dependent variable when all independent variables are set to zero.

model.coef_

y_pred=model.predict(test_x)

mse = mean_squared_error(test_y, y_pred)
r2 = r2_score(test_y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
classifier_lr = LogisticRegression(random_state = 0)
classifier_lr.fit(train_x, train_y)

y_pred1 = classifier_lr.predict(test_x)

accuracy2 = accuracy_score(test_y, y_pred1)

print("Accuracy Score:", accuracy2)

from sklearn.metrics import confusion_matrix,f1_score
cm = confusion_matrix( y_pred1,test_y)

from sklearn.metrics import classification_report
print(classification_report(test_y, y_pred1))

f2=f1_score(test_y,y_pred1,average='micro')

f2

"""DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier
classifier= DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(train_x, train_y)

y_pred2=classifier.predict(test_x)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred2, test_y)
print('Decision Tree Model accuracy score: {0:0.4f}'.format(accuracy_score(test_y, y_pred2)))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_y, y_pred2)
plt.figure(figsize=(15,15))
sns.heatmap(cm, annot=True, fmt=".0f", linewidths=.5, square = True, cmap = 'Blues');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Confusion Matrix - score:'+str(accuracy_score(test_y,y_pred2))
plt.title(all_sample_title, size = 15);
plt.show()

print(classification_report(test_y, y_pred2))

"""## RANDOM FOREST CLASSIFIER"""

from sklearn.ensemble import RandomForestClassifier
classifier_rf= RandomForestClassifier(n_estimators= 10, criterion="entropy")
classifier_rf.fit(train_x, train_y)

y_pred3= classifier_rf.predict(test_x)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred3, test_y)
print('Random Forest Model accuracy score: {0:0.4f}'.format(accuracy_score(test_y, y_pred3)))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_y, y_pred3)
plt.figure(figsize=(15,15))
sns.heatmap(cm, annot=True, fmt=".0f", linewidths=.5, square = True, cmap = 'Blues');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Confusion Matrix - score:'+str(accuracy_score(test_y,y_pred3))
plt.title(all_sample_title, size = 15);
plt.show()

print(classification_report(test_y, y_pred3))

"""SUPPORT VECTOR MACHINE


"""

from sklearn import svm

x=x.reshape(-1,1)

crop.dtypes
feature_df=crop[['N','P','K','temperature','humidity','ph','rainfall']]
x=np.asarray(feature_df)
y=np.asarray(crop['crop_num'])
X[0:5]

from sklearn.model_selection import train_test_split

train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.25)

from sklearn import svm

model=svm.SVC(kernel='linear',gamma='auto',C=2)

model.fit(train_x,train_y)
y_pred=model.predict(test_x)

from sklearn.metrics import classification_report
print(classification_report(test_y,y_pred))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_y, y_pred)
plt.figure(figsize=(15,15))
sns.heatmap(cm, annot=True, fmt=".0f", linewidths=.5, square = True, cmap = 'Blues');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Confusion Matrix - score:'+str(accuracy_score(test_y,y_pred))
plt.title(all_sample_title, size = 15);
plt.show()

acc=accuracy_score(y_pred,test_y)
acc

f1sv=f1_score(y_pred,test_y,average='micro')
f1sv

"""KMeans

"""

crop_x=crop.iloc[:,4:6]
crop_x.head()

wcss=[]
for i in range(1,11):
  kmeans=KMeans(n_clusters=i,init='k-means++',random_state=42)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_)
plt.plot(range(1,11),wcss)
plt.title("The elbow")
plt.ylabel('WCSS')
plt.show()

crop.plot(x='humidity',y='ph',style='o')
plt.title('humidity vs ph')
plt.xlabel('humidity')
plt.ylabel('ph')
plt.show()

#crop'''kmeans=KMeans(n_clusters=8,random_state=40)
#cropkmeans.fit(x_scaled)'''

#print(kmeans.cluster_centers_)

#print(kmeans.labels_)
#crop["kluster"]=kmeans.labels_

#crop'''output=plt.scatter(x_scaled[:,0],x_scaled[:,1],s=100,c=crop.kluster,marker="x",alpha=1,)
#cropcventers=kmeans.cluster_centers_
#cropplt.scatter(centers[:,0],centers[:,1],c='red',s=100,marker="s");
#cropplt.title("Hasil Klustering K-Means")
#cropplt.colorbar(output)
#cropplt.show()'''

#actual
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load your crop-related dataset (make sure it contains features like temperature, rainfall, etc.)
# Replace 'your_data.csv' with the path to your dataset.
data = pd.read_csv('/content/Crop_recommendation.csv')

# Select relevant features for clustering
features = data[['humidity', 'rainfall']]

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(features)

# Number of clusters (i.e., recommended crops)
num_clusters = 4

# Initialize K-Means model
kmeans = KMeans(n_clusters=num_clusters, random_state=0)

# Fit the model to your data
kmeans.fit(scaled_data)

# Assign cluster labels to each data point
data['Cluster'] = kmeans.labels_

crops = {
    0: 'Wheat',
    1: 'Rice',
    2: 'Maize',
}

# Function to recommend a crop based on temperature and rainfall
def recommend_crop(humidity, rainfall):
    scaled_input = scaler.transform([[humidity, rainfall]])
    cluster = kmeans.predict(scaled_input)[0]
    recommended_crop = crops[cluster]
    return recommended_crop

# Example usage
humidity = 35
rainfall = 52
recommended_crop = recommend_crop(humidity, rainfall)
print(f"Recommended crop for humidity {humidity}°C and rainfall {rainfall}mm:     {recommended_crop}")
from sklearn.metrics import silhouette_score

# Assuming you have already fitted the K-Means model to your data
silhouette_avg = silhouette_score(scaled_data, kmeans.labels_)
print(f"\n Silhouette Score: {silhouette_avg}\n")

ac=accuracy_score(y_pred,test_y)
ac

f1

"""Performance Component Anlysis

"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA


features = crop[['temperature', 'rainfall']]  # Add more features as needed

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(features)

# Initialize and fit the PCA model
n_components = 2  # Set the number of components you want
pca = PCA(n_components=n_components)
pca_result = pca.fit_transform(scaled_data)

# You can access the principal components and their explained variance like this:
explained_variance = pca.explained_variance_ratio_
print("Explained Variance:", explained_variance)

# You can also access the principal component loadings (coefficients) if needed:
component_loadings = pca.components_
print("Principal Component Loadings:", component_loadings)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Split the data into training and testing sets
# Train a classifier on the reduced data
clf = RandomForestClassifier()
clf.fit(train_x, train_y)

# Make predictions on the test set
predictions = clf.predict(test_x)

# Calculate accuracy
accuracy = accuracy_score(test_y, predictions)
print(f"Accuracy: {accuracy}")

pcaf=PCA(n_components = 2)
pcaf.fit(train_x, train_y)

predictions=clf.predict(test_x)
f1pca=f1_score(test_y,predictions,average='micro')
print(f1pca)

"""ANN

"""

# Check the data shapes and types
print("Shape of train_x:", train_x.shape)
print("Shape of train_y:", train_y.shape)
print("Shape of test_x:", test_x.shape)
print("Shape of test_y:", test_y.shape)

# Check the data types
print("Data type of train_x:", train_x.dtype)
print("Data type of train_y:", train_y.dtype)
print("Data type of test_x:", test_x.dtype)
print("Data type of test_y:", test_y.dtype)

# Ensure class labels are properly encoded
print("Unique class labels in train_y:", np.unique(train_y))
print("Unique class labels in test_y:", np.unique(test_y))

'''# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras
from tensorflow.keras import layers


# Preprocess the data
 X = crop.drop('crop_num', axis=1)  # Features
y = crop['crop_num']  # Target variable'''

# Encode the target variable
'''label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build the neural network model
model = keras.Sequential([
    layers.Input(shape=(X_train.shape[1],)),  # Input layer
    layers.Dense(64, activation='relu'),  # Hidden layer with ReLU activation
    layers.Dense(32, activation='relu'),  # Hidden layer with ReLU activation
    layers.Dense(len(label_encoder.classes_), activation='softmax')  # Output layer with softmax activation
])


# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

# Make predictions
predictions = model.predict(X_test)
predicted_labels = np.argmax(predictions, axis=1)
predicted_crops = label_encoder.inverse_transform(predicted_labels)

# Example: Given new features 'new_features' for crop recommendation'''
'''new_features = np.array([[...]])  # Replace with the features for recommendation
new_features = scaler.transform(new_features)
recommendation = model.predict(new_features)
recommended_crop_label = np.argmax(recommendation)
recommended_crop = label_encoder.inverse_transform([recommended_crop_label])

print(f'Recommended Crop: {recommended_crop[0]}')'''

#f1ann=f1_score(y_test,predictions.argmax(axis=1),average='micro')
#print(f1ann)